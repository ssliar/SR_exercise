{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "from os.path import exists, join, basename\n",
    "from os import remove\n",
    "from six.moves import urllib\n",
    "import tarfile\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath).convert('YCbCr')\n",
    "    y, _, _ = img.split()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, input_transform=None, target_transform=None):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.image_filenames = [join(image_dir, x) for x in listdir(image_dir) if is_image_file(x)]\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "    def __getitem__(self, index):\n",
    "        input_image = load_img(self.image_filenames[index])\n",
    "        target = input_image.copy()\n",
    "        if self.input_transform:\n",
    "            input_image = self.input_transform(input_image)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        return input_image, target\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_bsd300(dest=\"./dataset\"):\n",
    "    output_image_dir = join(dest, \"BSDS300/images\")\n",
    "\n",
    "    if not exists(output_image_dir):\n",
    "        url = \"http://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz\"\n",
    "        print(\"downloading url \", url)\n",
    "\n",
    "        data = urllib.request.urlopen(url)\n",
    "\n",
    "        file_path = join(dest, basename(url))\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(data.read())\n",
    "\n",
    "        print(\"Extracting data\")\n",
    "        with tarfile.open(file_path) as tar:\n",
    "            for item in tar:\n",
    "                tar.extract(item, dest)\n",
    "\n",
    "        remove(file_path)\n",
    "\n",
    "    return output_image_dir\n",
    "\n",
    "\n",
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)\n",
    "\n",
    "\n",
    "def input_transform(crop_size, upscale_factor):\n",
    "    return Compose([\n",
    "        CenterCrop(crop_size),\n",
    "        Scale(crop_size // upscale_factor),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def target_transform(crop_size):\n",
    "    return Compose([\n",
    "        CenterCrop(crop_size),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_training_set(upscale_factor):\n",
    "    root_dir = download_bsd300()\n",
    "    train_dir = join(root_dir, \"train\")\n",
    "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "    return DatasetFromFolder(train_dir,\n",
    "                             input_transform=input_transform(crop_size, upscale_factor),\n",
    "                             target_transform=target_transform(crop_size))\n",
    "\n",
    "\n",
    "def get_test_set(upscale_factor):\n",
    "    root_dir = download_bsd300()\n",
    "    test_dir = join(root_dir, \"test\")\n",
    "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "    return DatasetFromFolder(test_dir,\n",
    "                             input_transform=input_transform(crop_size, upscale_factor),\n",
    "                             target_transform=target_transform(crop_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_channels, base_channels, num_residuals):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.input_conv = nn.Sequential(nn.Conv2d(num_channels, base_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                        nn.ReLU(inplace=True))\n",
    "\n",
    "        conv_blocks = []\n",
    "        for _ in range(num_residuals):\n",
    "            conv_blocks.append(nn.Sequential(nn.Conv2d(base_channels, base_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                             nn.ReLU(inplace=True)))\n",
    "        self.residual_layers = nn.Sequential(*conv_blocks)\n",
    "\n",
    "        self.output_conv = nn.Conv2d(base_channels, num_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.input_conv(x)\n",
    "        x = self.residual_layers(x)\n",
    "        x = self.output_conv(x)\n",
    "        x = torch.add(x, residual)\n",
    "        return x\n",
    "\n",
    "    def weight_init(self):\n",
    "        for m in self._modules:\n",
    "            weights_init_kaiming(m)\n",
    "\n",
    "\n",
    "def weights_init_kaiming(m):\n",
    "    class_name = m.__class__.__name__\n",
    "    if class_name.find('Linear') != -1:\n",
    "        nn.init.kaiming_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif class_name.find('Conv2d') != -1:\n",
    "        nn.init.kaiming_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif class_name.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif class_name.find('Norm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_and_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "TOTAL_BAR_LENGTH = 80\n",
    "LAST_T = time.time()\n",
    "BEGIN_T = LAST_T\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global LAST_T, BEGIN_T\n",
    "    if current == 0:\n",
    "        BEGIN_T = time.time()  # Reset for new bar.\n",
    "\n",
    "    current_len = int(TOTAL_BAR_LENGTH * (current + 1) / total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - current_len) - 1\n",
    "\n",
    "    sys.stdout.write(' %d/%d' % (current + 1, total))\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(current_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    current_time = time.time()\n",
    "    step_time = current_time - LAST_T\n",
    "    LAST_T = current_time\n",
    "    total_time = current_time - BEGIN_T\n",
    "\n",
    "    time_used = '  Step: %s' % format_time(step_time)\n",
    "    time_used += ' | Tot: %s' % format_time(total_time)\n",
    "    if msg:\n",
    "        time_used += ' | ' + msg\n",
    "\n",
    "    msg = time_used\n",
    "    sys.stdout.write(msg)\n",
    "\n",
    "    if current < total - 1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "# return the formatted time\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    seconds_final = int(seconds)\n",
    "    seconds = seconds - seconds_final\n",
    "    millis = int(seconds*1000)\n",
    "    output = ''\n",
    "    time_index = 1\n",
    "    if days > 0:\n",
    "        output += str(days) + 'D'\n",
    "        time_index += 1\n",
    "    if hours > 0 and time_index <= 2:\n",
    "        output += str(hours) + 'h'\n",
    "        time_index += 1\n",
    "    if minutes > 0 and time_index <= 2:\n",
    "        output += str(minutes) + 'm'\n",
    "        time_index += 1\n",
    "    if seconds_final > 0 and time_index <= 2:\n",
    "        output += str(seconds_final) + 's'\n",
    "        time_index += 1\n",
    "    if millis > 0 and time_index <= 2:\n",
    "        output += str(millis) + 'ms'\n",
    "        time_index += 1\n",
    "    if output == '':\n",
    "        output = '0ms'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from math import log10\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "#from VDSR.model import Net\n",
    "#from misc import progress_bar\n",
    "from PIL import Image\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VDSRTrainer(object):\n",
    "    def __init__(self, training_loader, testing_loader):\n",
    "        self.model = None\n",
    "        self.lr = 0.02\n",
    "        self.nEpochs = 10\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        #self.GPU_IN_USE = torch.cuda.is_available()\n",
    "        self.seed = 111\n",
    "        self.upscale_factor = 4\n",
    "        self.training_loader = training_loader\n",
    "        self.testing_loader = testing_loader\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = Net(num_channels=1, base_channels=64, num_residuals=4)\n",
    "        self.model.weight_init()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        #if self.GPU_IN_USE:\n",
    "            #torch.cuda.manual_seed(self.seed)\n",
    "            #cudnn.benchmark = True\n",
    "            #self.model.cuda()\n",
    "            #self.criterion.cuda()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.scheduler = lr_scheduler.MultiStepLR(self.optimizer, milestones=[50, 75, 100], gamma=0.5)  # lr decay\n",
    "\n",
    "    def img_preprocess(self, data, interpolation='bicubic'):\n",
    "        if interpolation == 'bicubic':\n",
    "            interpolation = Image.BICUBIC\n",
    "        elif interpolation == 'bilinear':\n",
    "            interpolation = Image.BILINEAR\n",
    "        elif interpolation == 'nearest':\n",
    "            interpolation = Image.NEAREST\n",
    "\n",
    "        size = list(data.shape)\n",
    "\n",
    "        if len(size) == 4:\n",
    "            target_height = int(size[2] * self.upscale_factor)\n",
    "            target_width = int(size[3] * self.upscale_factor)\n",
    "            out_data = torch.FloatTensor(size[0], size[1], target_height, target_width)\n",
    "            for i, img in enumerate(data):\n",
    "                transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                transforms.Scale((target_width, target_height), interpolation=interpolation),\n",
    "                                                transforms.ToTensor()])\n",
    "\n",
    "                out_data[i, :, :, :] = transform(img)\n",
    "            return out_data\n",
    "        else:\n",
    "            target_height = int(size[1] * self.upscale_factor)\n",
    "            target_width = int(size[2] * self.upscale_factor)\n",
    "            transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                            transforms.Scale((target_width, target_height), interpolation=interpolation),\n",
    "                                            transforms.ToTensor()])\n",
    "            return transform(data)\n",
    "\n",
    "    def save(self):\n",
    "        model_out_path = \"VDSR_model_path.pth\"\n",
    "        torch.save(self.model, model_out_path)\n",
    "        print(\"Checkpoint saved to {}\".format(model_out_path))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        data: [torch.cuda.FloatTensor], 4 batches: [64, 64, 64, 8]\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for batch_num, (data, target) in enumerate(self.training_loader):\n",
    "            data = self.img_preprocess(data)  # resize input image size\n",
    "            #if self.GPU_IN_USE:\n",
    "                #data, target = Variable(data).cuda(), Variable(target).cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(self.model(data), target)\n",
    "            train_loss += loss.data[0]\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            progress_bar(batch_num, len(self.training_loader), 'Loss: %.4f' % (train_loss / (batch_num + 1)))\n",
    "\n",
    "        print(\"    Average Loss: {:.4f}\".format(train_loss / len(self.training_loader)))\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        data: [torch.cuda.FloatTensor], 10 batches: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        avg_psnr = 0\n",
    "        for batch_num, (data, target) in enumerate(self.testing_loader):\n",
    "            data = self.img_preprocess(data)  # resize input image size\n",
    "            #if self.GPU_IN_USE:\n",
    "                #data, target = Variable(data).cuda(), Variable(target).cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            prediction = self.model(data)\n",
    "            mse = self.criterion(prediction, target)\n",
    "            psnr = 10 * log10(1 / mse.data[0])\n",
    "            avg_psnr += psnr\n",
    "            progress_bar(batch_num, len(self.testing_loader), 'PSNR: %.4f' % (avg_psnr / (batch_num + 1)))\n",
    "\n",
    "        print(\"    Average PSNR: {:.4f} dB\".format(avg_psnr / len(self.testing_loader)))\n",
    "\n",
    "    def validate(self):\n",
    "        self.build_model()\n",
    "        for epoch in range(1, self.nEpochs + 1):\n",
    "            print(\"\\n===> Epoch {} starts:\".format(epoch))\n",
    "            self.train()\n",
    "            self.test()\n",
    "            self.scheduler.step(epoch)\n",
    "            if epoch == self.nEpochs:\n",
    "                self.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Epoch 1 starts:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c3e3df2ace88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtesting_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVDSRTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-a41162d3455b>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnEpochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n===> Epoch {} starts:\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a41162d3455b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# resize input image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;31m#if self.GPU_IN_USE:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m#data, target = Variable(data).cuda(), Variable(target).cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a41162d3455b>\u001b[0m in \u001b[0;36mimg_preprocess\u001b[0;34m(self, data, interpolation)\u001b[0m\n\u001b[1;32m     47\u001b[0m                                                 transforms.ToTensor()])\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mout_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.5/site-packages/torchvision/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.5/site-packages/torchvision/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0moh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "train_set = get_training_set(4)\n",
    "test_set = get_test_set(4)\n",
    "training_data_loader = data.DataLoader(dataset=train_set, batch_size=2, shuffle=True)\n",
    "testing_data_loader = data.DataLoader(dataset=test_set, batch_size=2, shuffle=False)\n",
    "model = VDSRTrainer(training_data_loader, testing_data_loader)\n",
    "model.validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
