{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "from os.path import exists, join, basename\n",
    "from os import remove\n",
    "from six.moves import urllib\n",
    "import tarfile\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath).convert('YCbCr')\n",
    "    y, _, _ = img.split()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, input_transform=None, target_transform=None):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.image_filenames = [join(image_dir, x) for x in listdir(image_dir) if is_image_file(x)]\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "    def __getitem__(self, index):\n",
    "        input_image = load_img(self.image_filenames[index])\n",
    "        target = input_image.copy()\n",
    "        if self.input_transform:\n",
    "            input_image = self.input_transform(input_image)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        return input_image, target\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_bsd300(dest=\"./dataset\"):\n",
    "    output_image_dir = join(dest, \"BSDS300/images\")\n",
    "\n",
    "    if not exists(output_image_dir):\n",
    "        url = \"http://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz\"\n",
    "        print(\"downloading url \", url)\n",
    "\n",
    "        data = urllib.request.urlopen(url)\n",
    "\n",
    "        file_path = join(dest, basename(url))\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(data.read())\n",
    "\n",
    "        print(\"Extracting data\")\n",
    "        with tarfile.open(file_path) as tar:\n",
    "            for item in tar:\n",
    "                tar.extract(item, dest)\n",
    "\n",
    "        remove(file_path)\n",
    "\n",
    "    return output_image_dir\n",
    "\n",
    "\n",
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)\n",
    "\n",
    "\n",
    "def input_transform(crop_size, upscale_factor):\n",
    "    return Compose([\n",
    "        CenterCrop(crop_size),\n",
    "        Scale(crop_size // upscale_factor),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def target_transform(crop_size):\n",
    "    return Compose([\n",
    "        CenterCrop(crop_size),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_training_set(upscale_factor):\n",
    "    root_dir = download_bsd300()\n",
    "    train_dir = join(root_dir, \"train\")\n",
    "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "    return DatasetFromFolder(train_dir,\n",
    "                             input_transform=input_transform(crop_size, upscale_factor),\n",
    "                             target_transform=target_transform(crop_size))\n",
    "\n",
    "\n",
    "def get_test_set(upscale_factor):\n",
    "    root_dir = download_bsd300()\n",
    "    test_dir = join(root_dir, \"test\")\n",
    "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "    return DatasetFromFolder(test_dir,\n",
    "                             input_transform=input_transform(crop_size, upscale_factor),\n",
    "                             target_transform=target_transform(crop_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_channels, base_channel, num_recursions):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_recursions = num_recursions\n",
    "        # embedding layer\n",
    "        self.embedding_layer = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, base_channel, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_channel, base_channel, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # conv block of inference layer\n",
    "        self.conv_block = nn.Sequential(nn.Conv2d(base_channel, base_channel, kernel_size=3, stride=1, padding=1),\n",
    "                                        nn.ReLU(inplace=True))\n",
    "\n",
    "        # reconstruction layer\n",
    "        self.reconstruction_layer = nn.Sequential(\n",
    "            nn.Conv2d(base_channel, base_channel, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(base_channel, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        # initial w\n",
    "        self.w_init = torch.ones(self.num_recursions) / self.num_recursions\n",
    "        self.w = Variable(self.w_init.cuda(), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embedding layer\n",
    "        h0 = self.embedding_layer(x)\n",
    "\n",
    "        # recursions\n",
    "        h = [h0]\n",
    "        for d in range(self.num_recursions):\n",
    "            h.append(self.conv_block(h[d]))\n",
    "\n",
    "        y_d_ = []\n",
    "        out_sum = 0\n",
    "        for d in range(self.num_recursions):\n",
    "            y_d_.append(self.reconstruction_layer(h[d+1]))\n",
    "            out_sum += torch.mul(y_d_[d], self.w[d])\n",
    "        out_sum = torch.mul(out_sum, 1.0 / (torch.sum(self.w)))\n",
    "\n",
    "        # skip connection\n",
    "        final_out = torch.add(out_sum, x)\n",
    "\n",
    "        return y_d_, final_out\n",
    "\n",
    "    def weight_init(self):\n",
    "        for m in self._modules:\n",
    "            weights_init_kaiming(m)\n",
    "\n",
    "def weights_init_kaiming(m):\n",
    "    class_name = m.__class__.__name__\n",
    "    if class_name.find('Linear') != -1:\n",
    "        torch.nn.init.kaiming_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif class_name.find('Conv2d') != -1:\n",
    "        torch.nn.init.kaiming_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif class_name.find('ConvTranspose2d') != -1:\n",
    "        torch.nn.init.kaiming_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif class_name.find('Norm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_and_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "TOTAL_BAR_LENGTH = 80\n",
    "LAST_T = time.time()\n",
    "BEGIN_T = LAST_T\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global LAST_T, BEGIN_T\n",
    "    if current == 0:\n",
    "        BEGIN_T = time.time()  # Reset for new bar.\n",
    "\n",
    "    current_len = int(TOTAL_BAR_LENGTH * (current + 1) / total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - current_len) - 1\n",
    "\n",
    "    sys.stdout.write(' %d/%d' % (current + 1, total))\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(current_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    current_time = time.time()\n",
    "    step_time = current_time - LAST_T\n",
    "    LAST_T = current_time\n",
    "    total_time = current_time - BEGIN_T\n",
    "\n",
    "    time_used = '  Step: %s' % format_time(step_time)\n",
    "    time_used += ' | Tot: %s' % format_time(total_time)\n",
    "    if msg:\n",
    "        time_used += ' | ' + msg\n",
    "\n",
    "    msg = time_used\n",
    "    sys.stdout.write(msg)\n",
    "\n",
    "    if current < total - 1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "# return the formatted time\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    seconds_final = int(seconds)\n",
    "    seconds = seconds - seconds_final\n",
    "    millis = int(seconds*1000)\n",
    "    output = ''\n",
    "    time_index = 1\n",
    "    if days > 0:\n",
    "        output += str(days) + 'D'\n",
    "        time_index += 1\n",
    "    if hours > 0 and time_index <= 2:\n",
    "        output += str(hours) + 'h'\n",
    "        time_index += 1\n",
    "    if minutes > 0 and time_index <= 2:\n",
    "        output += str(minutes) + 'm'\n",
    "        time_index += 1\n",
    "    if seconds_final > 0 and time_index <= 2:\n",
    "        output += str(seconds_final) + 's'\n",
    "        time_index += 1\n",
    "    if millis > 0 and time_index <= 2:\n",
    "        output += str(millis) + 'ms'\n",
    "        time_index += 1\n",
    "    if output == '':\n",
    "        output = '0ms'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from math import log10\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "#from DRCN.model import Net\n",
    "#from misc import progress_bar\n",
    "from PIL import Image\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DRCNTrainer(object):\n",
    "    def __init__(self,training_loader, testing_loader):\n",
    "        self.model = None\n",
    "        self.lr = 0.01\n",
    "        self.nEpochs = 20\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        #self.GPU_IN_USE = torch.cuda.is_available()\n",
    "        self.seed = 111\n",
    "        self.upscale_factor = 4\n",
    "        self.training_loader = training_loader\n",
    "        self.testing_loader = testing_loader\n",
    "\n",
    "        # DRCN setup\n",
    "        self.momentum = 0.9\n",
    "        self.weight_decay = 0.0001\n",
    "        self.loss_alpha = 1.0\n",
    "        self.loss_alpha_zero_epoch = 25\n",
    "        self.loss_alpha_decay = self.loss_alpha / self.loss_alpha_zero_epoch\n",
    "        self.loss_beta = 0.001\n",
    "        self.num_recursions = 16\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = Net(num_channels=1, base_channel=64, num_recursions=self.num_recursions)\n",
    "        self.model.weight_init()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        #if self.GPU_IN_USE:\n",
    "            #torch.cuda.manual_seed(self.seed)\n",
    "            #cudnn.benchmark = True\n",
    "            #self.model.cuda()\n",
    "            #self.criterion.cuda()\n",
    "\n",
    "        # setup optimizer and scheduler\n",
    "        param_groups = [{'params': list(self.model.parameters())}]\n",
    "        param_groups += [{'params': [self.model.w]}]\n",
    "        self.optimizer = optim.Adam(param_groups, lr=self.lr)\n",
    "        self.scheduler = lr_scheduler.MultiStepLR(self.optimizer, milestones=[50, 75, 100], gamma=0.5)  # lr decay\n",
    "\n",
    "    def img_preprocess(self, data, interpolation='bicubic'):\n",
    "        if interpolation == 'bicubic':\n",
    "            interpolation = Image.BICUBIC\n",
    "        elif interpolation == 'bilinear':\n",
    "            interpolation = Image.BILINEAR\n",
    "        elif interpolation == 'nearest':\n",
    "            interpolation = Image.NEAREST\n",
    "\n",
    "        size = list(data.shape)\n",
    "\n",
    "        if len(size) == 4:\n",
    "            target_height = int(size[2] * self.upscale_factor)\n",
    "            target_width = int(size[3] * self.upscale_factor)\n",
    "            out_data = torch.FloatTensor(size[0], size[1], target_height, target_width)\n",
    "            for i, img in enumerate(data):\n",
    "                transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                transforms.Scale((target_width, target_height), interpolation=interpolation),\n",
    "                                                transforms.ToTensor()])\n",
    "\n",
    "                out_data[i, :, :, :] = transform(img)\n",
    "            return out_data\n",
    "        else:\n",
    "            target_height = int(size[1] * self.upscale_factor)\n",
    "            target_width = int(size[2] * self.upscale_factor)\n",
    "            transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                            transforms.Scale((target_width, target_height), interpolation=interpolation),\n",
    "                                            transforms.ToTensor()])\n",
    "            return transform(data)\n",
    "\n",
    "    def save(self):\n",
    "        model_out_path = \"DRCN_model_path.pth\"\n",
    "        torch.save(self.model, model_out_path)\n",
    "        print(\"Checkpoint saved to {}\".format(model_out_path))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        data: [torch.cuda.FloatTensor], 4 batches: [64, 64, 64, 8]\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for batch_num, (data, target) in enumerate(self.training_loader):\n",
    "            data = self.img_preprocess(data)  # resize input image size\n",
    "            #if self.GPU_IN_USE:\n",
    "                #data, target = Variable(data).cuda(), Variable(target).cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            self.optimizer.zero_grad()\n",
    "            target_d, output = self.model(data)\n",
    "\n",
    "            # loss1\n",
    "            loss_1 = 0\n",
    "            for d in range(self.num_recursions):\n",
    "                loss_1 += (self.criterion(target_d[d], target) / self.num_recursions)\n",
    "\n",
    "            # loss2\n",
    "            loss_2 = self.criterion(output, target)\n",
    "\n",
    "            # regularization\n",
    "            reg_term = 0\n",
    "            for theta in self.model.parameters():\n",
    "                reg_term += torch.mean(torch.sum(theta ** 2))\n",
    "\n",
    "            # total loss\n",
    "            loss = self.loss_alpha * loss_1 + (1 - self.loss_alpha) * loss_2 + self.loss_beta * reg_term\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss += loss.data[0]\n",
    "            self.optimizer.step()\n",
    "            progress_bar(batch_num, len(self.training_loader), 'Loss: %.4f' % (train_loss / (batch_num + 1)))\n",
    "\n",
    "        print(\"    Average Loss: {:.4f}\".format(train_loss / len(self.training_loader)))\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        data: [torch.cuda.FloatTensor], 10 batches: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        avg_psnr = 0\n",
    "        for batch_num, (data, target) in enumerate(self.testing_loader):\n",
    "            data = self.img_preprocess(data)  # resize input image size\n",
    "            #if self.GPU_IN_USE:\n",
    "                #data, target = Variable(data).cuda(), Variable(target).cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            _, prediction = self.model(data)\n",
    "            mse = self.criterion(prediction, target)\n",
    "            psnr = 10 * log10(1 / mse.data[0])\n",
    "            avg_psnr += psnr\n",
    "            progress_bar(batch_num, len(self.testing_loader), 'PSNR: %.4f' % (avg_psnr / (batch_num + 1)))\n",
    "\n",
    "        print(\"    Average PSNR: {:.4f} dB\".format(avg_psnr / len(self.testing_loader)))\n",
    "\n",
    "    def validate(self):\n",
    "        self.build_model()\n",
    "        for epoch in range(1, self.nEpochs + 1):\n",
    "            print(\"\\n===> Epoch {} starts:\".format(epoch))\n",
    "            self.loss_alpha = max(0.0, self.loss_alpha - self.loss_alpha_decay)\n",
    "            self.train()\n",
    "            self.test()\n",
    "            self.scheduler.step(epoch)\n",
    "            if epoch == self.nEpochs:\n",
    "                self.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Epoch 1 starts:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f437aeb3d43d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtesting_data_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDRCNTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-d6f1f1719664>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n===> Epoch {} starts:\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_alpha\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_alpha_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d6f1f1719664>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# resize input image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;31m#if self.GPU_IN_USE:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;31m#data, target = Variable(data).cuda(), Variable(target).cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d6f1f1719664>\u001b[0m in \u001b[0;36mimg_preprocess\u001b[0;34m(self, data, interpolation)\u001b[0m\n\u001b[1;32m     59\u001b[0m                                                 transforms.ToTensor()])\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mout_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.5/site-packages/torchvision/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.5/site-packages/torchvision/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0moh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "train_set = get_training_set(4)\n",
    "test_set = get_test_set(4)\n",
    "training_data_loader = data.DataLoader(dataset=train_set, batch_size=16, shuffle=True)\n",
    "testing_data_loader = data.DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "model = DRCNTrainer(training_data_loader, testing_data_loader)\n",
    "model.validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
