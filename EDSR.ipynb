{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "from os.path import exists, join, basename\n",
    "from os import remove\n",
    "from six.moves import urllib\n",
    "import tarfile\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath).convert('YCbCr')\n",
    "    y, _, _ = img.split()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, input_transform=None, target_transform=None):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.image_filenames = [join(image_dir, x) for x in listdir(image_dir) if is_image_file(x)]\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "    def __getitem__(self, index):\n",
    "        input_image = load_img(self.image_filenames[index])\n",
    "        target = input_image.copy()\n",
    "        if self.input_transform:\n",
    "            input_image = self.input_transform(input_image)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        return input_image, target\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_bsd300(dest=\"./dataset\"):\n",
    "    output_image_dir = join(dest, \"BSDS300/images\")\n",
    "\n",
    "    if not exists(output_image_dir):\n",
    "        url = \"http://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz\"\n",
    "        print(\"downloading url \", url)\n",
    "\n",
    "        data = urllib.request.urlopen(url)\n",
    "\n",
    "        file_path = join(dest, basename(url))\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(data.read())\n",
    "\n",
    "        print(\"Extracting data\")\n",
    "        with tarfile.open(file_path) as tar:\n",
    "            for item in tar:\n",
    "                tar.extract(item, dest)\n",
    "\n",
    "        remove(file_path)\n",
    "\n",
    "    return output_image_dir\n",
    "\n",
    "\n",
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)\n",
    "\n",
    "\n",
    "def input_transform(crop_size, upscale_factor):\n",
    "    return Compose([\n",
    "        CenterCrop(crop_size),\n",
    "        Scale(crop_size // upscale_factor),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def target_transform(crop_size):\n",
    "    return Compose([\n",
    "        CenterCrop(crop_size),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_training_set(upscale_factor):\n",
    "    root_dir = download_bsd300()\n",
    "    train_dir = join(root_dir, \"train\")\n",
    "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "    return DatasetFromFolder(train_dir,\n",
    "                             input_transform=input_transform(crop_size, upscale_factor),\n",
    "                             target_transform=target_transform(crop_size))\n",
    "\n",
    "\n",
    "def get_test_set(upscale_factor):\n",
    "    root_dir = download_bsd300()\n",
    "    test_dir = join(root_dir, \"test\")\n",
    "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "    return DatasetFromFolder(test_dir,\n",
    "                             input_transform=input_transform(crop_size, upscale_factor),\n",
    "                             target_transform=target_transform(crop_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_channels, base_channel, upscale_factor, num_residuals):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.input_conv = nn.Conv2d(num_channels, base_channel, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        resnet_blocks = []\n",
    "        for _ in range(num_residuals):\n",
    "            resnet_blocks.append(ResnetBlock(base_channel, kernel=3, stride=1, padding=1))\n",
    "        self.residual_layers = nn.Sequential(*resnet_blocks)\n",
    "\n",
    "        self.mid_conv = nn.Conv2d(base_channel, base_channel, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        upscale = []\n",
    "        for _ in range(int(math.log2(upscale_factor))):\n",
    "            upscale.append(PixelShuffleBlock(base_channel, base_channel, upscale_factor=2))\n",
    "        self.upscale_layers = nn.Sequential(*upscale)\n",
    "\n",
    "        self.output_conv = nn.Conv2d(base_channel, num_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def weight_init(self, mean=0.0, std=0.02):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_conv(x)\n",
    "        residual = x\n",
    "        x = self.residual_layers(x)\n",
    "        x = self.mid_conv(x)\n",
    "        x = torch.add(x, residual)\n",
    "        x = self.upscale_layers(x)\n",
    "        x = self.output_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, num_channel, kernel=3, stride=1, padding=1):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channel, num_channel, kernel, stride, padding)\n",
    "        self.conv2 = nn.Conv2d(num_channel, num_channel, kernel, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(num_channel)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.bn(self.conv1(x))\n",
    "        x = self.activation(x)\n",
    "        x = self.bn(self.conv2(x))\n",
    "        x = torch.add(x, residual)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PixelShuffleBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, upscale_factor, kernel=3, stride=1, padding=1):\n",
    "        super(PixelShuffleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel * upscale_factor ** 2, kernel, stride, padding)\n",
    "        self.ps = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ps(self.conv(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_and_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "TOTAL_BAR_LENGTH = 80\n",
    "LAST_T = time.time()\n",
    "BEGIN_T = LAST_T\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global LAST_T, BEGIN_T\n",
    "    if current == 0:\n",
    "        BEGIN_T = time.time()  # Reset for new bar.\n",
    "\n",
    "    current_len = int(TOTAL_BAR_LENGTH * (current + 1) / total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - current_len) - 1\n",
    "\n",
    "    sys.stdout.write(' %d/%d' % (current + 1, total))\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(current_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    current_time = time.time()\n",
    "    step_time = current_time - LAST_T\n",
    "    LAST_T = current_time\n",
    "    total_time = current_time - BEGIN_T\n",
    "\n",
    "    time_used = '  Step: %s' % format_time(step_time)\n",
    "    time_used += ' | Tot: %s' % format_time(total_time)\n",
    "    if msg:\n",
    "        time_used += ' | ' + msg\n",
    "\n",
    "    msg = time_used\n",
    "    sys.stdout.write(msg)\n",
    "\n",
    "    if current < total - 1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "# return the formatted time\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    seconds_final = int(seconds)\n",
    "    seconds = seconds - seconds_final\n",
    "    millis = int(seconds*1000)\n",
    "    output = ''\n",
    "    time_index = 1\n",
    "    if days > 0:\n",
    "        output += str(days) + 'D'\n",
    "        time_index += 1\n",
    "    if hours > 0 and time_index <= 2:\n",
    "        output += str(hours) + 'h'\n",
    "        time_index += 1\n",
    "    if minutes > 0 and time_index <= 2:\n",
    "        output += str(minutes) + 'm'\n",
    "        time_index += 1\n",
    "    if seconds_final > 0 and time_index <= 2:\n",
    "        output += str(seconds_final) + 's'\n",
    "        time_index += 1\n",
    "    if millis > 0 and time_index <= 2:\n",
    "        output += str(millis) + 'ms'\n",
    "        time_index += 1\n",
    "    if output == '':\n",
    "        output = '0ms'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from math import log10\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "#from EDSR.model import Net\n",
    "#from misc import progress_bar\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EDSRTrainer(object):\n",
    "    def __init__(self,training_loader, testing_loader):\n",
    "        self.model = None\n",
    "        self.lr = 0.01\n",
    "        self.nEpochs = 10\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        #self.GPU_IN_USE = torch.cuda.is_available()\n",
    "        self.seed = 111\n",
    "        self.upscale_factor = 4\n",
    "        self.training_loader = training_loader\n",
    "        self.testing_loader = testing_loader\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = Net(num_channels=1, upscale_factor=self.upscale_factor, base_channel=64, num_residuals=4)\n",
    "        self.model.weight_init(mean=0.0, std=0.02)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        #if self.GPU_IN_USE:\n",
    "            #torch.cuda.manual_seed(self.seed)\n",
    "            #cudnn.benchmark = True\n",
    "            #self.model.cuda()\n",
    "            #self.criterion.cuda()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999), eps=1e-8)\n",
    "        self.scheduler = lr_scheduler.MultiStepLR(self.optimizer, milestones=[50, 75, 100], gamma=0.5)  # lr decay\n",
    "\n",
    "    def save(self):\n",
    "        model_out_path = \"EDSR_model_path.pth\"\n",
    "        torch.save(self.model, model_out_path)\n",
    "        print(\"Checkpoint saved to {}\".format(model_out_path))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        data: [torch.cuda.FloatTensor], 4 batches: [64, 64, 64, 8]\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for batch_num, (data, target) in enumerate(self.training_loader):\n",
    "            #if self.GPU_IN_USE:\n",
    "                #data, target = Variable(data).cuda(), Variable(target).cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(self.model(data), target)\n",
    "            train_loss += loss.data[0]\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            progress_bar(batch_num, len(self.training_loader), 'Loss: %.4f' % (train_loss / (batch_num + 1)))\n",
    "\n",
    "        print(\"    Average Loss: {:.4f}\".format(train_loss / len(self.training_loader)))\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        data: [torch.cuda.FloatTensor], 10 batches: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        avg_psnr = 0\n",
    "        for batch_num, (data, target) in enumerate(self.testing_loader):\n",
    "            #if self.GPU_IN_USE:\n",
    "                #data, target = Variable(data).cuda(), Variable(target).cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            prediction = self.model(data)\n",
    "            mse = self.criterion(prediction, target)\n",
    "            psnr = 10 * log10(1 / mse.data[0])\n",
    "            avg_psnr += psnr\n",
    "            progress_bar(batch_num, len(self.testing_loader), 'PSNR: %.4f' % (avg_psnr / (batch_num + 1)))\n",
    "\n",
    "        print(\"    Average PSNR: {:.4f} dB\".format(avg_psnr / len(self.testing_loader)))\n",
    "\n",
    "    def validate(self):\n",
    "        self.build_model()\n",
    "        for epoch in range(1, self.nEpochs + 1):\n",
    "            print(\"\\n===> Epoch {} starts:\".format(epoch))\n",
    "            self.train()\n",
    "            self.test()\n",
    "            self.scheduler.step(epoch)\n",
    "            if epoch == self.nEpochs:\n",
    "                self.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Epoch 1 starts:\n",
      " 200/200 [================================================================================>]  Step: 1s250ms | Tot: 4m14s | Loss: 11.724537\n",
      "    Average Loss: 11.7245\n",
      " 100/100 [================================================================================>]  Step: 621ms | Tot: 1m11s | PSNR: 3.6201318\n",
      "    Average PSNR: 3.6201 dB\n",
      "\n",
      "===> Epoch 2 starts:\n",
      " 200/200 [================================================================================>]  Step: 1s650ms | Tot: 5m29s | Loss: 0.08815\n",
      "    Average Loss: 0.0881\n",
      " 100/100 [================================================================================>]  Step: 502ms | Tot: 1m341ms | PSNR: 7.4663\n",
      "    Average PSNR: 7.4663 dB\n",
      "\n",
      "===> Epoch 3 starts:\n",
      " 200/200 [================================================================================>]  Step: 1s138ms | Tot: 4m9s | Loss: 0.086064\n",
      "    Average Loss: 0.0860\n",
      " 100/100 [================================================================================>]  Step: 467ms | Tot: 48s159ms | PSNR: 6.7496\n",
      "    Average PSNR: 6.7496 dB\n",
      "\n",
      "===> Epoch 4 starts:\n",
      " 200/200 [================================================================================>]  Step: 1s237ms | Tot: 3m52s | Loss: 0.07719\n",
      "    Average Loss: 0.0771\n",
      " 100/100 [================================================================================>]  Step: 490ms | Tot: 47s917ms | PSNR: 7.9861\n",
      "    Average PSNR: 7.9861 dB\n",
      "\n",
      "===> Epoch 5 starts:\n",
      " 200/200 [================================================================================>]  Step: 1s164ms | Tot: 3m49s | Loss: 0.07068\n",
      "    Average Loss: 0.0706\n",
      " 100/100 [================================================================================>]  Step: 471ms | Tot: 47s832ms | PSNR: 6.2282\n",
      "    Average PSNR: 6.2282 dB\n",
      "\n",
      "===> Epoch 6 starts:\n",
      " 200/200 [================================================================================>]  Step: 1s148ms | Tot: 3m49s | Loss: 0.06664\n",
      "    Average Loss: 0.0666\n",
      " 100/100 [================================================================================>]  Step: 478ms | Tot: 52s517ms | PSNR: 7.4792\n",
      "    Average PSNR: 7.4792 dB\n",
      "\n",
      "===> Epoch 7 starts:\n",
      " 200/200 [================================================================================>]  Step: 1s132ms | Tot: 4m14s | Loss: 0.06922\n",
      "    Average Loss: 0.0692\n",
      " 100/100 [================================================================================>]  Step: 475ms | Tot: 48s259ms | PSNR: 6.5426\n",
      "    Average PSNR: 6.5426 dB\n",
      "\n",
      "===> Epoch 8 starts:\n",
      " 200/200 [================================================================================>]  Step: 1s146ms | Tot: 3m55s | Loss: 0.06735\n",
      "    Average Loss: 0.0673\n",
      " 100/100 [================================================================================>]  Step: 527ms | Tot: 48s293ms | PSNR: 6.7144\n",
      "    Average PSNR: 6.7144 dB\n",
      "\n",
      "===> Epoch 9 starts:\n",
      " 200/200 [================================================================================>]  Step: 1s167ms | Tot: 3m51s | Loss: 0.06424\n",
      "    Average Loss: 0.0642\n",
      " 100/100 [================================================================================>]  Step: 486ms | Tot: 48s9ms | PSNR: 9.3442\n",
      "    Average PSNR: 9.3442 dB\n",
      "\n",
      "===> Epoch 10 starts:\n",
      " 200/200 [================================================================================>]  Step: 1s135ms | Tot: 3m50s | Loss: 0.06503\n",
      "    Average Loss: 0.0650\n",
      " 100/100 [================================================================================>]  Step: 499ms | Tot: 48s121ms | PSNR: 7.5957\n",
      "    Average PSNR: 7.5957 dB\n",
      "Checkpoint saved to EDSR_model_path.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liar/anaconda3/envs/python3/lib/python3.5/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/liar/anaconda3/envs/python3/lib/python3.5/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type ResnetBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/liar/anaconda3/envs/python3/lib/python3.5/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type PixelShuffleBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "train_set = get_training_set(4)\n",
    "test_set = get_test_set(4)\n",
    "training_data_loader = data.DataLoader(dataset=train_set, batch_size=1, shuffle=False)\n",
    "testing_data_loader = data.DataLoader(dataset=test_set, batch_size=1, shuffle=False)\n",
    "model = EDSRTrainer(training_data_loader, testing_data_loader)\n",
    "model.validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
